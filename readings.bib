@article{tollenaere-looss-pouget-brunie-guillon-cohen-sadayappan-rastello:2023, author = {Tollenaere, Nicolas and Iooss, Guillaume and Pouget, St\'{e}phane and Brunie, Hugo and Guillon, Christophe and Cohen, Albert and Sadayappan, P. and Rastello, Fabrice}, title = {Autotuning Convolutions Is Easier Than You Think}, year = {2023}, issue_date = {June 2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {20}, number = {2}, issn = {1544-3566}, url = {https://doi.org/10.1145/3570641}, doi = {10.1145/3570641}, abstract = {A wide range of scientific and machine learning applications depend on highly optimized implementations of tensor computations. Exploiting the full capacity of a given processor architecture remains a challenging task, due to the complexity of the microarchitectural features that come into play when seeking near-peak performance. Among the state-of-the-art techniques for loop transformations for performance optimization, AutoScheduler [Zheng et al. 2020a] tends to outperform other systems. It often yields higher performance as compared to vendor libraries, but takes a large number of runs to converge, while also involving a complex training environment. In this article, we define a structured configuration space that enables much faster convergence to high-performance code versions, using only random sampling of candidates. We focus on two-dimensional convolutions on CPUs. Compared to state-of-the-art libraries, our structured search space enables higher performance for typical tensor shapes encountered in convolution stages in deep learning pipelines. Compared to auto-tuning code generators like AutoScheduler, it prunes the search space while increasing the density of efficient implementations. We analyze the impact on convergence speed and performance distribution, on two Intel x86 processors and one ARM AArch64 processor. We match or outperform the performance of the state-of-the-art oneDNN library and TVMâ€™s AutoScheduler, while reducing the autotuning effort by at least an order of magnitude.}, journal = {ACM Trans. Archit. Code Optim.}, month = {mar}, articleno = {20}, numpages = {24}, keywords = {optimisation space, microkernel, Code generation, convolution} }

@inproceedings{phothilimthana-sabne-sarda-murthy-zhou-angermueller-burrows-roy-mandke-wang-hechtman-Wang-xu-kaufman,
  author       = {Phothilimthana, Phitchaya Mangpo and Sabne, Amit and Sarda, Nikhil and Murthy, Karthik Srinivasa and Zhou, Yanqi and Angermueller, Christof and Burrows, Mike and Roy, Sudip and Mandke, Ketan  and Farahani, Rezsa and Wang, Yu Emma and Ilbeyi, Berkin and Hechtman, Blake A. and Roune, Bjarke and Wang, Shen and Xu, Yuanzhong and Kaufman, Samuel J. },
  editor       = {Jaejin Lee and
                  Albert Cohen},
  title        = {A Flexible Approach to Autotuning Multi-Pass Machine Learning Compilers},
  booktitle    = {30th International Conference on Parallel Architectures and Compilation
                  Techniques, {PACT} 2021, Atlanta, GA, USA, September 26-29, 2021},
  pages        = {1--16},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/PACT52795.2021.00008},
  doi          = {10.1109/PACT52795.2021.00008},
  timestamp    = {Wed, 27 Oct 2021 09:02:18 +0200},
  biburl       = {https://dblp.org/rec/conf/IEEEpact/PhothilimthanaS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
